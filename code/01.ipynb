{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01eb06a0-8fd4-450e-9166-7af5e63754c8",
   "metadata": {},
   "source": [
    "# DSI Team Starting Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bd30a-2ded-4ff3-85ba-fee68698ce72",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8de4fa-8057-4ff9-be1f-5c35af5c1525",
   "metadata": {},
   "source": [
    "DSI Phase I:\n",
    "- Cleaned and Analyzed Historical CFPB Agreement Data looking to understand offered APRs\n",
    "    - Team found initial analysis lacking in meaning due to the nature of the APR values reported in the agreements\n",
    "        - The Reported APRs were listed as a \"middle point\" across customer segments per plan per agreement.\n",
    "        - This ultiamtely meant that the APR value was already a range or a value from a range rather than the discreet values themselves.\n",
    "    - Later found out from data stewards that the historical data is non-verifiable, meaning the reported APRs were hard to relate to the actual offerings at that time.\n",
    "    - Data Teams Presentations:\n",
    "        - <a href=\"https://github.com/chrisJoyceDS/top_sprint_data/blob/main/code/CFPB_Data.ipynb\"> Jupyter Notebook Repo </a>\n",
    "        - <a href=\"https://docs.google.com/presentation/d/16vBsI7b5Fw_0tF1UQK4qW2dn32v66PwcC3VZ959mx4c/edit#slide=id.p\"> Presentation to the Team </a>\n",
    "        - <a href=\"https://docs.google.com/presentation/d/18uy5qkw0zQTg-SA-ceXDKOI2dCI4RLN-IjSQXfIwCIM/edit#slide=id.p\"> Presentation to the CFPB </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99658f25-8138-4df5-b649-951ca6dc9d43",
   "metadata": {},
   "source": [
    "DSI Phase II:\n",
    "- After having it confirmed that the historical data was innacurate and far from reality, the team learned of and used the updated dataset provided by the CFPB\n",
    "- This data was from July 2022 to December 2022 and had a variety of updated information with respect to the historical data\n",
    "- Unfortunately there is no data dictionary for this dataset, and we will have to interpet their values based on the column descriptions\n",
    "- for the sake of the team, we left most of the column names intact, and update a few where we thought it was best/relevant\n",
    "- Please follow us along or Select \"Jump to Clustering\" below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe7f63-fabf-44c9-8565-2e22bc92ca77",
   "metadata": {},
   "source": [
    "[Jump to Clustering](#Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56491cc-2501-4fb2-8476-a2a5513aef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import re\n",
    "import missingno as msno\n",
    "\n",
    "# imputing\n",
    "from sklearn.impute import KNNImputer\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01744db-eb29-4338-b7cb-90a552e564e8",
   "metadata": {},
   "source": [
    "- The dataset data doesn't fall on the usual A0 cell, we needed to adjust the pandas reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affb976-528f-4e1c-90ed-65e8105e11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_read = 'B:FO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d81fd9-717c-48fc-a865-d1382d831f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/cfpb_updated_tccp.xlsx\",skiprows=9,usecols=columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2374e0-ec7c-47e0-b236-80fc5fa3efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514a6d7-ed69-410b-a1fb-1bb0ac2ba136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f7f97-bbda-4be4-9ca0-2287b076ccd7",
   "metadata": {},
   "source": [
    "- Key differences between this data set and the historical data set\n",
    "    - This is not timeseries, it is each institution's plan data as of December 31 of the participants in the survey\n",
    "    - Phase I dataset was (rows:9556,columns:12)\n",
    "    - more dense of a dataset\n",
    "- let's check out the null landscape below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae517f-bd42-41fb-8374-68733dd23e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df.sample(250))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe340d-93f5-4967-a38b-26fe1acc4fd6",
   "metadata": {},
   "source": [
    "- What you are seeing is a graphical representation of the data within the new data set:\n",
    "    - Black means data\n",
    "    - White means no data\n",
    "- With more fields collected, comes the even possibility of not collecting it.\n",
    "- Below is the start of our initial clean process. Because of the look of the dataset above, and the sheer amount of initial features we have (170) we are going to make some swift cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bb1c2-f826-472e-bab8-05757ab0a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609166fd-6b61-4e6e-b409-e32b387852c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of null values in each column\n",
    "null_percentage = (df.isnull().sum() / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16f9f0-1ea6-4b86-b407-ab559bfc2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(null_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cddc8f-e41c-4f8b-babe-c4a8082baf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fce74a-7259-4532-89fe-5994c1f3f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold\n",
    "threshold = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1b0c4-b64e-41fd-a900-881a55170616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column names to drop where the null percentage is greater than or equal to the threshold\n",
    "columns_to_drop = null_percentage[null_percentage >= threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f06e2-82b3-433f-b268-6b3b5b99c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ace69-82ea-4939-bc76-ec0dc8bf87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns\n",
    "df.drop(columns=columns_to_drop,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b2252-6f26-4982-a877-4874e6dbc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcc0e5-62e0-4963-b821-54ed985818bf",
   "metadata": {},
   "source": [
    "- Above we:\n",
    "    - Calculated the null percentage for each column of the 170\n",
    "    - set a threshold of 90 percent null as the cut off\n",
    "    - removed all columns that met that threshould and above\n",
    "    - resulting in dropping 68 columns\n",
    "- You can see the impact below, and it's much better, but we still have 102 features, which means we will be greedier in our selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6630591-260a-42cc-aca5-9ae185dfee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df.sample(250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a699529-a2d6-4945-b362-43476d880526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27dcfd-d107-4129-90cc-078f7e9001b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get remaining column data types\n",
    "col_types = df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468627d0-8db3-483d-8beb-b6738d8d87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out numerical and object columns\n",
    "num_cols = col_types[col_types != 'object'].index.tolist()\n",
    "obj_cols = col_types[col_types == 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2067d62-ec7f-4cbd-a3e8-a9b940b0cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245cc31-ab2e-43bb-9898-7ecc66dc5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[obj_cols].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bf4d1-9a51-47ec-8217-7d132a6e61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols].isnull().sum().sum(), df[obj_cols].isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c8ffa-3f42-4bf7-83ee-8556417a3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-Calculate the percentage of null values in each column\n",
    "null_percentage = (df.isnull().sum() / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d794c5f-3688-49a3-af72-0da8c6abaa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage.sort_values(ascending=False)[null_percentage >= 59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aef606-8cd1-48eb-ac29-b4008151d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional columns to drop\n",
    "cols_to_drop = null_percentage[null_percentage >= 60].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301afee-62b8-4340-a25d-d5ac768c6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d25aa-bdad-4d33-8844-12b7723839c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop additional columns\n",
    "# drop the columns\n",
    "df.drop(columns=cols_to_drop,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2297d75-1582-49c4-be5b-22da42692a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5dcc66-e4b4-4dfc-9998-71ff77a81263",
   "metadata": {},
   "source": [
    "- Same exercise, new threshold, this time anything that was greater or equal to 59% null\n",
    "- Now we have 68 Columns (5.5 times the features we had with historical btw) with a much better look of data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0744e8-7281-4f0d-a8d9-780c894f0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df.sample(250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64395616-df27-4dd7-800f-8075e509f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d3be6-873a-41a3-afb8-f9debebcb83f",
   "metadata": {},
   "source": [
    "- Something you will have noticed, if you read our initial analysis, is that there are less visualizations for this data set\n",
    "- There will be more going forward, but for now we have prioritized creating aggregate cards for the Software Engineering Team to be able to start building\n",
    "- From this point we will be doing a mix of cleaning, feature engineering, preprocessing, and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751199a-bbbd-4640-82fe-036144229bdc",
   "metadata": {},
   "source": [
    "---\n",
    "## MVP Goal\n",
    "\n",
    "- Create Aggregate cards based on the targeted credit tiers \n",
    "- Capture APR, Grace Periods, Late Fees, Rewards, and Annual Fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44518bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Targeted Credit Tiers\"] = df[\"Targeted Credit Tiers\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5df065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Targeted Credit Tiers\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e7de2",
   "metadata": {},
   "source": [
    "- split the strings on the ';'\n",
    "- for the non technical this just makes it easier for us to interact with\n",
    "- Goes from:\n",
    "    - good credit (credit scores from 620 to 719); great credit (credit score of 720 or greater) \n",
    "    - to\n",
    "    - ['good credit (credit scores from 620 to 719)',\n",
    " 'great credit (credit score of 720 or greater)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Targeted Credit Tiers\"] = df[\"Targeted Credit Tiers\"].str.split(\"; \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784451f",
   "metadata": {},
   "source": [
    "- Create three columns that align with the credit score group.\n",
    "- if the word is found in a given list of options for a specific group, report True, else False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Separate Columns for each bucket\n",
    "df['Poor_Fair_Group'] = df['Targeted Credit Tiers'].apply(lambda x: any(re.search(r'poor', s) for s in x))\n",
    "df['Good_Credit_Group'] = df['Targeted Credit Tiers'].apply(lambda x: any(re.search(r'good', s) for s in x))\n",
    "df['Great_Credit_Group'] = df['Targeted Credit Tiers'].apply(lambda x: any(re.search(r'great', s) for s in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19faf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Poor_Fair_Group','Good_Credit_Group','Great_Credit_Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert boolean into numerical values\n",
    "df['Poor_Fair_Group'] = df['Poor_Fair_Group'].astype(int)\n",
    "df['Good_Credit_Group'] = df['Good_Credit_Group'].astype(int)\n",
    "df['Great_Credit_Group'] = df['Great_Credit_Group'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Poor_Fair_Group','Good_Credit_Group','Great_Credit_Group']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba940f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad119ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a0731",
   "metadata": {},
   "source": [
    "Immediately, some of these columns are aparently redundant. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Purchase APR Offered?', 'Purchase APR Vary by Balance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4dbb3",
   "metadata": {},
   "source": [
    "Whether an APR is offered is already represented in `Purchase APR Vary by Balance` because in cases where an APR is not offered, there will be a NaN. That said, I am going to have to impute these NaNs before I can cluster, so this relationship will be lost, but would be retained by `Purchase APR Offered?` if I decide to keep this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907bf7b",
   "metadata": {},
   "source": [
    "Some other features I assume won't be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7624b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Website for Consumer','Telephone Number for Consumers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9d0ff",
   "metadata": {},
   "source": [
    "Is there a real relationship between credit score and the URL/phone number of the company offering a credit card? Probably not. I'll drop these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba628d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Website for Consumer','Telephone Number for Consumers'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7162bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68036d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Good Credit', 'Good_Credit_Group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079517b",
   "metadata": {},
   "source": [
    "I'm not sure what's going on here^ </br>\n",
    "Good_Credit_Group was a feature engineered by Chris. Good Credit appears to be one of the original features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Other Fees', 'Additional Fees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Other Fees'] == 'No'][['Other Fees', 'Additional Fees']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ceee9a",
   "metadata": {},
   "source": [
    "It appears that \"Additional Fees\" is dependent on \"Other Fees\", where the former being non-null depends on the latter being non-null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff14f7d",
   "metadata": {},
   "source": [
    "As with the APR features examined above, this means these two features are redundant, but I'm not sure if I want to get rid of one of them as the original relationship will be lost when I impute the nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Late Fees?', 'Late Fee Types', 'Amount (Dollars) - Late Payment Fee',\n",
    "       'Late Fee Six Month Billing Cycle', 'Late Fee Policy Details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Late Fee Types'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f461c",
   "metadata": {},
   "source": [
    "There is clearly something messed up with this feature. It appears to contain the text data used to collect responses i.e. these are the prompts used to get banks to imput info about their late fees. Probably, I'll just drop this feature as I don't know what else to do with it and it doesn't appear to contain useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711026aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Late Fee Types'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dba79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Grace Period Offered?', 'Balance Transfer Grace Period', 'Grace Period']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14dddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Grace Period Offered?'] == 'No'][['Grace Period Offered?', 'Balance Transfer Grace Period', 'Grace Period']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c462cda",
   "metadata": {},
   "source": [
    "So, the balance transfer grace period is not a subset of the overall grace period offered. In row 581 above, there is no grace period offered according to the first column, but aparently this does not stop there from being a balance transfer grace period. Based on this, I don't understand what the first column is refering to. The third column does not appear to be in reference to the balance transfer grace period as there is no information (length of grace period) specified for 581 either. Although all of this is assuming the data is complete and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Over Limit Fees?', 'Other Fees', 'Additional Fees', 'Other Fee Name',\n",
    "       'Other Fee Amount', 'Other Fee Explanation', 'Other Fee Name.1',\n",
    "       'Other Fee Amount.1', 'Other Fee Explanation.1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Other Fees', 'Other Fee Name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0bd96",
   "metadata": {},
   "source": [
    "As mentioned several times, this dataset is full of redundant features where the first column is a boolean, and the second column actually contains the data, but only the first column is 'Yes'. With these, I would like to get rid of as many redundant features as I can, and I'm realizing that I can probably drop the first column by setting NaNs in the second column to either 0 or str type \"None\" depending on the dtype. This way, I retain the original meaning contained in the first column, I have a straightforward fill technique and I can drop many redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b909e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Other Fee Name'].replace(np.NaN, 'None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Other Fee Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06805609",
   "metadata": {},
   "source": [
    "As examined before, 'Additional Fees' is a subset of 'Other Fees'. 'Additional Fees' can only be 'Yes' when 'Other Fees' is 'Yes'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bb6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Other Fees', 'Additional Fees']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff749385",
   "metadata": {},
   "source": [
    "Based on this, it seems reasonable to assume that 'NaN' values in 'Additional Fees' can be filled with  'No' and still retain the original meaning. I am assuming that 'NaN' values in 'Additional Fees' are a product of it being a subset of 'Other Fees'. If I drop 'Other Fees' and fill NaNs in 'Additional Fees' with 'No', the original relationship seen in the table above will be lost, but I assume that this relationship was not valid to begin with, and NaNs in 'Additional Fees' are actually supposed to be 'No'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Additional Fees'].replace(np.NaN, 'No', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd527525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Additional Fees'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d7284",
   "metadata": {},
   "source": [
    "**Examining the two features I just modified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Other Fees', 'Other Fee Name', 'Additional Fees']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215927a",
   "metadata": {},
   "source": [
    "Now we can see the product of my imputation decision. The first two columns have a clear relationship, but Additional Fees is only Yes in cases where Other Fees is Yes. Even then, Additional Fees is only Yes in a fraction of cases. I think this makes sense - there are a variety of fees in the dataset related to balance transfer, purchase transaction, etc. \n",
    "\n",
    "Other fees captures those fees specific to certain banks, with the name contained in Other Fee Name. Additional Fees is an even smaller subset of cases not recorded in Other Fees for some reason. \n",
    "\n",
    "Ideally, I would like to combine Additional Fees and Other Fees into one feature. I could have 0 be No, 1 be Other Fees == Yes and 2 be Other Fees AND Additional Fees == Yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c92c25",
   "metadata": {},
   "source": [
    "The issue is that there is no clearly labeled Additional Fee Amount feaure, and capturing the actual numeric amount of the fee would be better than simple OHE. Maybe Other Fee Amount.1 contains info for Additional Fees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494249ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Other Fee Amount', 'Other Fee Amount.1', 'Additional Fees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Additional Fees'] == 'Yes'][['Other Fee Amount.1', 'Additional Fees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02942b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Additional Fees'] == 'No'][['Other Fee Amount.1', 'Additional Fees']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea135c",
   "metadata": {},
   "source": [
    "Based on the two tables above, it appears that Other Fee Amount.1 contains the cash amount of Additional Fees. Excellent, I can now combine the two Other Fee Amount features into one, rendering several features redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb39db",
   "metadata": {},
   "source": [
    "Before doing this though, I need to convert NaNs in Other Fee Amount.1 to 0 so that this addition works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Other Fee Amount.1'].replace(np.NaN, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202720d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['other_fee_amount'] = df['Other Fee Amount'] + df['Other Fee Amount.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['other_fee_amount', 'Other Fee Amount', 'Other Fee Amount.1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc87235",
   "metadata": {},
   "source": [
    "Now that I think about it, it's not really relevant whether or not a fee exists if its value is 0. I thought this would help capture some significant information in the features I'm aiming to drop, it it doesn't. I'll simply convert NaNs in my new feature to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61665980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['other_fee_amount'].replace(np.NaN, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['other_fee_amount', 'Other Fee Amount', 'Other Fee Amount.1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1801d3",
   "metadata": {},
   "source": [
    "There are a lot of other features related to other fees regarding their name, explaination, etc, but I don't think these will help our model. All of the relevant information is now condensed into other_fee_amount. I'll drop the unhelpful features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Other Fees', 'Additional Fees', 'Other Fee Name',\n",
    "       'Other Fee Amount', 'Other Fee Explanation', 'Other Fee Name.1',\n",
    "       'Other Fee Amount.1', 'Other Fee Explanation.1'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969d275",
   "metadata": {},
   "source": [
    "**Revisiting other redundant features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Grace Period Offered?', 'Balance Transfer Grace Period', 'Grace Period']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9793a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Grace Period Offered?'] == 'No'][['Grace Period Offered?','Grace Period']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Grace Period Offered?'] == 'Yes'][['Grace Period Offered?','Grace Period']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474150c1",
   "metadata": {},
   "source": [
    "Grace Period Offered is redundant because NaNs in Grace Period simply correspond to a No in the former column. Balance Transfer Grace Period must be a subset type of grace period that only some cards offer. I'll simply drop the first column and convert NaNs in the third to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef725ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Grace Period'].replace(np.NaN, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Grace Period Offered?'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13168cce",
   "metadata": {},
   "source": [
    "**Removing features I don't think will help the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df06f8b",
   "metadata": {},
   "source": [
    "Contact Information Types, Report Date and Created Date don't appear to be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62637d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[11:-11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dcee18",
   "metadata": {},
   "source": [
    "**Feature engineering late fee info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58823e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Late Fees?',\n",
    "       'Amount (Dollars) - Late Payment Fee',\n",
    "       'Late Fee Six Month Billing Cycle', 'Late Fee Policy Details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Late Fee Policy Details'].isna() == False][['Late Fees?',\n",
    "       'Amount (Dollars) - Late Payment Fee',\n",
    "       'Late Fee Six Month Billing Cycle', 'Late Fee Policy Details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['Late Fee Policy Details'].isna() == False]['Late Fee Policy Details'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Late Fee Policy Details'].isna() == False]['Late Fee Policy Details'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb609db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Late Fee Policy Details'].isna() == False]['Late Fee Policy Details'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5231940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Late Fee Policy Details'].isna() == False]['Late Fee Policy Details'].unique()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Late Fee Policy Details'].isna() == False]['Late Fee Policy Details'].unique()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b87975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Late Fee Policy Details'].notna()) \n",
    "                & (df['Amount (Dollars) - Late Payment Fee'].notna())][[\n",
    "                'Amount (Dollars) - Late Payment Fee',\n",
    "                'Late Fee Policy Details']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f4042",
   "metadata": {},
   "source": [
    "Here are some examples where both the late fee amount and the details are filled out. I can use this as a reference for how to impute missing late fee amounts using the details feature. In general, there's a dollar specified in the details which is identical to the late fee amount. Here is an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Late Fee Policy Details'].notna()) \n",
    "                & (df['Amount (Dollars) - Late Payment Fee'].notna())][[\n",
    "                'Amount (Dollars) - Late Payment Fee',\n",
    "                'Late Fee Policy Details']].loc[78]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f5dcf",
   "metadata": {},
   "source": [
    "The details state \"We charge up to $35.00.\" There would be no way to know that the late fee is assessed at `$29` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad328022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Late Fee Policy Details'].notna()) \n",
    "                & (df['Amount (Dollars) - Late Payment Fee'].notna())]['Late Fee Policy Details'][62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560e7f5",
   "metadata": {},
   "source": [
    "There are multiple dollar amounts listed here. The lower number is the original late fee, the second is an additional fee assessed after six months of non-payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Late Fee Policy Details'].isna() == False][['Late Fees?',\n",
    "       'Amount (Dollars) - Late Payment Fee',\n",
    "       'Late Fee Six Month Billing Cycle', 'Late Fee Policy Details']].loc[62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adfa73f",
   "metadata": {},
   "source": [
    "I'll try to extract the dollar amount for the late fees from the description. If there's more than one dollar sign in the description, I'll set the higher amount to be the six month fee, assuming the amounts are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39078fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Late Fee Policy Details'].isna() == False]['Late Fee Policy Details'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2507d3e",
   "metadata": {},
   "source": [
    "This approach won't really work in this instance. ^ The $27 appears to be assessed immediately, not after a period of 6 months. Still, I think the general idea is the same regardless of the time period, as there are multiple tiers of fees being assessed depending on the borrower's debt status. I'll manually fix this error, and accept that there may be similar errors for now. I'll try another approach with kNN imputation of this feature once the dataset is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6e414",
   "metadata": {},
   "source": [
    "Finding the index for relevant cases where there is a late fee needing imputation and there are policy details I can use to impute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_index = df[(df['Amount (Dollars) - Late Payment Fee'].isna() == True)\n",
    "   & (df['Late Fee Policy Details'].notna())].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[policy_index][[\n",
    "                'Amount (Dollars) - Late Payment Fee',\n",
    "                'Late Fee Policy Details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fee_imputer():\n",
    "    fees_df = pd.DataFrame(index=range(len(df)), columns=['fee', 'late_fee'])\n",
    "    for i in range(len(df)):\n",
    "        if i in policy_index:\n",
    "            policy = df['Late Fee Policy Details'][i].split('$')[1:]\n",
    "            temp_df = pd.DataFrame(index=[i], columns=['fee', 'late_fee'])\n",
    "\n",
    "            if len(policy) == 2:\n",
    "                policy[0] = policy[0].replace(',', '').strip('.').split('.')[0].split(' ')[0]\n",
    "                policy[1] = policy[1].replace(',', '').strip('.').split('.')[0].split(' ')[0]\n",
    "\n",
    "                temp_df['fee'] = int(policy[0])\n",
    "                temp_df['late_fee'] = int(policy[1])\n",
    "\n",
    "                if policy[0] == policy[1]:\n",
    "                    temp_df['late_fee'] = 0\n",
    "\n",
    "                fees_df.loc[i] = temp_df.loc[i]\n",
    "\n",
    "            elif len(policy) == 1:\n",
    "                policy[0] = policy[0].replace(',', '').strip('.').split('.')[0].split(' ')[0]\n",
    "                temp_df['fee'] = int(policy[0])\n",
    "                temp_df['late_fee'] = 0\n",
    "\n",
    "                fees_df.loc[i] = temp_df.loc[i]\n",
    "        else: \n",
    "            fees_df.loc[i] = [0, 0]\n",
    "    return fees_df\n",
    "\n",
    "result = fee_imputer()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[policy_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8440b01",
   "metadata": {},
   "source": [
    "In general this has worked, but there are some errors due to some numbers lacking dollar signs. Splitting using numerics won't work because there are many percentages throughout. I'll simply have to impute these another way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb877959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Late Fee Policy Details'][599]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be60ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['fee'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Late Fee Policy Details'][58]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb8310",
   "metadata": {},
   "source": [
    "^ Again, there is no clear way to account for this using my function. I'll kNN impute once I clean all the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0cf0a",
   "metadata": {},
   "source": [
    "I'll add in my new values by replacing all the NaNs in the existing columns with 0, then I'll add the new values to them. The new NaNs will cause the 0s to be set to NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cff7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Amount (Dollars) - Late Payment Fee','Late Fee Six Month Billing Cycle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9179fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = ['Amount (Dollars) - Late Payment Fee', 'Late Fee Six Month Billing Cycle']\n",
    "df.loc[:, columns_to_fill] = df[columns_to_fill].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Amount (Dollars) - Late Payment Fee','Late Fee Six Month Billing Cycle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount (Dollars) - Late Payment Fee'] + result['fee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fee'] = df['Amount (Dollars) - Late Payment Fee'] + result['fee']\n",
    "df['late_fee'] = df['Late Fee Six Month Billing Cycle'] + result['late_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af240f4d",
   "metadata": {},
   "source": [
    "The nulls have been maintained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a85466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['fee'].isna() == True][['fee','late_fee']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ace85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['fee'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f86757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Late Fees?',\n",
    "                   'Amount (Dollars) - Late Payment Fee',\n",
    "                   'Late Fee Six Month Billing Cycle', \n",
    "                   'Late Fee Policy Details'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2988d09",
   "metadata": {},
   "source": [
    "**Cleaning First 10 Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5, 0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 0:11].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d011a",
   "metadata": {},
   "source": [
    "**Examing APR features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Purchase APR Offered?', 'Purchase APR Vary by Balance',\n",
    "       'Purchase APR Index', 'Variable Rate Index', 'Index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Variable Rate Index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Variable Rate Index'] == 'Six-month T-bill'][['Variable Rate Index', 'Index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Variable Rate Index'] == 'Prime'][['Variable Rate Index', 'Index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8e0ba",
   "metadata": {},
   "source": [
    "Variable Rate Index does not appear to contain any information not already represented in Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Variable Rate Index'].isna() == True][['Variable Rate Index', 'Index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dec80b",
   "metadata": {},
   "source": [
    "The only difference is that nulls in Variable are not present in Index. This is a subtle difference, but I would prefer to preserve this relationship for now. I'll fill in nulls rather than dropping the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Variable Rate Index'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a970b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Variable Rate Index'].replace(np.NaN, 'None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23645edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Variable Rate Index', 'Index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446d445",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a49bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 0:11].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Purchase APR Offered?', 'Purchase APR Vary by Balance',\n",
    "       'Purchase APR Index', 'Variable Rate Index', 'Index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Purchase APR Offered?','Purchase APR Vary by Balance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Purchase APR Offered?'] == 'No'][['Purchase APR Offered?','Purchase APR Vary by Balance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2baa7b6",
   "metadata": {},
   "source": [
    "As with so many of these features, the second one is dependent on the first. Again, I'll simply fill in NaNs with 'None' to preserve the subset relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Purchase APR Vary by Balance'].replace(np.NaN, 'None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Purchase APR Vary by Balance'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar relationship with Purchase APR Index\n",
    "df[['Purchase APR Offered?','Purchase APR Index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e111bd",
   "metadata": {},
   "source": [
    "Same relationship. I'll maintain it by filling with 'None':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Purchase APR Index'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cf313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Purchase APR Index'].replace(np.NaN, 'None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55059305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Purchase APR Index'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 0:11].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Targeted Credit Tiers\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77344cf8",
   "metadata": {},
   "source": [
    "**Cleaning Features 11-21**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 11:22].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da741682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5, 11:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48503301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
